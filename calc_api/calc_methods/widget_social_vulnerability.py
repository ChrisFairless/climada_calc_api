import logging
import numpy as np
import pandas as pd
from celery import chain, chord, group, shared_task
from celery_singleton import Singleton

from climada.util.api_client import Client

from calc_api.config import ClimadaCalcApiConfig
from calc_api.vizz import schemas, schemas_widgets
from calc_api.vizz.text_social_vulnerability import generate_social_vulnerability_widget_text, generate_social_vulnerability_widget_text_no_data
from calc_api.calc_methods.calc_exposure import get_exposure, subset_dataframe_extent
from calc_api.vizz.schemas_widgets import SocialVulnerabilityWidgetData, SocialVulnerabilityWidgetRequest, SocialVulnerabilityWidgetResponse
from calc_api.job_management.job_management import database_job

conf = ClimadaCalcApiConfig()

LOGGER = logging.getLogger(__name__)
LOGGER.setLevel(getattr(logging, conf.LOG_LEVEL))


def widget_social_vulnerability(data: schemas_widgets.SocialVulnerabilityWidgetRequest):
    data.standardise()

    if data.location_poly and len(data.location_poly) != 4:
        LOGGER.warning('Ignoring location polygon data for soc vuln calculations: using location bbox')

    # TODO maybe add a check that social vulnerability data is available?

    chord_header = [
        get_soc_vuln_data.s(
            country_iso=data.geocoding.country_id,
            location_name=None,
            location_scale=data.location_scale,
            location_poly=data.geocoding.bbox),
        get_exposure.s(
            country=data.geocoding.country_id,
            exposure_type='people',
            impact_type=None,
            scenario_name='historical',
            scenario_growth='historical',
            scenario_year='2020',
            location_poly=data.geocoding.bbox,
            aggregation_scale=None,
            aggregation_method=None)
    ]
    chord_callback = combine_exposures_to_socvuln_widget.s(
        hazard_type=data.hazard_type,
        location_name=data.location_name,
        location_type=data.location_scale
    )

    res = chord(chord_header)(chord_callback)
    return res.id


@shared_task(base=Singleton)
@database_job   # TODO is this the right place to be cacheing?
def get_soc_vuln_data(country_iso, location_name, location_scale, location_poly, drop_zeroes=True):
    if location_scale == 'admin1':
        LOGGER.warning("Can't handle admin1 yet: ignoring")
        pass
    if location_poly and len(location_poly) != 4:
        LOGGER.warning("Can't handle polygons to extract social vulnerability data yet, provide a bounding box with length 4")
    exp = get_soc_vuln_from_api(country_iso)
    if not exp:
        return [
            {"value": None}
        ]

    if drop_zeroes:
        exp.gdf = exp.gdf[exp.gdf['value'] != 0]

    if location_poly and location_scale != 'country':
        exp = subset_exposure_extent(exp, location_poly, buffer=150)

    return [
        {"lat": float(row['latitude']),
         "lon": float(row['longitude']),
         "value": float(row['value'])}
        for _, row in exp.gdf.iterrows()
    ]


def get_soc_vuln_from_api(country_iso):
    request_properties = {
        'spatial_coverage': 'country',
        'country_iso3alpha': country_iso,
    }

    LOGGER.debug(
        f'Requesting relative wealth index data from Data API. Request properties: {request_properties}')
    client = Client()

    try:
        # TODO maybe make some of these parameters into settings
        soc_vuln = client.get_exposures(
            exposures_type='relative_wealth_litpop',
            properties=request_properties,
            status='preliminary',
            version='newest'
        )
    except Client.NoResult as err:
        LOGGER.warning(f'No social vulnerability data found for {country_iso}: returning None')
        return None

    return soc_vuln


def combine_exposures_to_socvuln_deciles(soc_vuln, exp):
    soc_vuln_df = pd.DataFrame(soc_vuln)
    exp_df = pd.DataFrame(exp)
    exp_df['lat'] = np.round(exp_df['lat'], 5)
    exp_df['lon'] = np.round(exp_df['lon'], 5)
    soc_vuln_df['lat'] = np.round(soc_vuln_df['lat'], 5)
    soc_vuln_df['lon'] = np.round(soc_vuln_df['lon'], 5)
    soc_vuln_df.rename(columns={'value': 'vuln'}, inplace=True)
    LOGGER.debug('SOCVULN details')
    LOGGER.debug(soc_vuln_df)
    LOGGER.debug(exp_df)
    exp_df = exp_df.merge(soc_vuln_df, on=['lat', 'lon'], how='left', copy=False)
    LOGGER.debug(exp_df)
    unmatched = np.isnan(exp_df['vuln'])
    if sum(unmatched) > 0:
        LOGGER.warning(f'Some exposure values did not match to a vulnerability, dropping these: {sum(unmatched)} / {len(exp_df)}')
        exp_df = exp_df[~unmatched]
    # exp.gdf.sort(['value'], inplace=True)
    # TODO i thought social vuln data was all in the [0,1] range. investigate
    vuln_min = min(exp_df['vuln'])
    vuln_max = max(exp_df['vuln'])
    bins = [vuln_min + i * (vuln_max - vuln_min) / 10 for i in range(10)]
    exp_df['decile'] = np.digitize(exp_df['vuln'], bins=bins)
    pop_by_decile = exp_df.groupby('decile').agg({'value': 'sum'}).reset_index()
    LOGGER.debug('socvuln by decile')
    LOGGER.debug(pop_by_decile)
    return pop_by_decile


@shared_task(base=Singleton)
def combine_exposures_to_socvuln_widget(
        exposure_list,
        hazard_type,
        location_name,
        location_type):
    soc_vuln, exp = exposure_list

    LOGGER.debug('SOCVULN')
    LOGGER.debug(soc_vuln[0:10])

    if len(soc_vuln) > 0 and soc_vuln[0]['value'] is not None:  # if there's soc vuln data for this place
        widget_text = generate_social_vulnerability_widget_text(
            soc_vuln,
            exp,
            hazard_type,
            location_name,
            location_type
        )

        pop_by_decile = combine_exposures_to_socvuln_deciles(soc_vuln, exp)

        widget_bars = [
            # TODO give country context
            schemas.ExposureBreakdownBar(
                label=f'Relative vulnerability breakdown for {location_name}',
                category_labels=[str(i) for i in pop_by_decile['decile']],
                values=list(pop_by_decile['value'])
            )
        ]

        widget_legend = schemas.CategoricalLegend(
            title=f'Relative vulnerability breakdown for {location_name}',
            units=None,
            items=[
                schemas.CategoricalLegendItem(
                    label=str(i),
                    slug=str(i),
                    value=5)
                for i in np.arange(1, 11)
            ]
        )

        widget_chart = schemas.ExposureBreakdown(
            items=widget_bars,
            legend=widget_legend
        )

    else:
        LOGGER.debug('No social vulnerability data received for widget.')
        widget_text = generate_social_vulnerability_widget_text_no_data(hazard_type)
        widget_chart = None

    widget_data = schemas_widgets.SocialVulnerabilityWidgetData(
        text=widget_text,
        chart=widget_chart
    )

    return schemas_widgets.SocialVulnerabilityWidgetResponse(
        data=widget_data,
        metadata={}
    )
